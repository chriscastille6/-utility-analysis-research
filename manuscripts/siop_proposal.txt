SUBMISSION TYPE

Poster

TITLE

Toward a More Useful Utility Analysis: A Literature Review and Web App

ABSTRACT

Scholars continue to discuss the merits and drawbacks of utility
analysis (UA) as a tool for influencing management decisions. In our
paper, we introduce the UA+ web app, illustrate its usefulness with
published use cases, and share plans for testing the UA+ app in field
settings.

SOCIAL MEDIA STATEMENT

Introducing the UA+ web app <https://uaplus.shinyapps.io/UAPlus> -- a
tool to help IO psychologists communicate the impact of our
interventions.

WORD COUNT

2994

**Toward a More Useful Utility Analysis: A Literature Review and
WebApp**

Scholars routinely bemoan the fact that management decisions are
suboptimal (Fisher et al., 2021; Highhouse, 2008; Rynes et al., 2002).
Causes have been posited such as the poor training of managers
(Highhouse, 2008; Rynes, 2012; Rynes et al., 2018), the widening
science-practice gap (Rynes, 2012; Pfeffer & Fong, 2002), and fetish for
theory in research that emphasizes complexity at the expense of validity
and utility in our published research (Götz & O'Boyle, 2023). To address
these concerns, educators in human resource management and
industrial-organizational psychology have been encouraged to improve
their training of both practitioners and scholarly researchers using
methods that emphasize validity and utility of theory-based
interventions (Sturman, 2000, 2001; Russell, 2022).

Utility analysis (UA) is one method of describing such evidence in
statistical, economic, and heuristic terms (i.e., quantity, quality, and
cost; Cascio et al., 2019). Recent examples of scholars using utility
analysis to communicate the value of theory-based interventions include
Avolio et al. (2010) who discuss the merits of a transformational
leadership intervention; Schmidt (2012) who highlights the economic
impact of goal setting; and Oprea et al. (2019) who highlight the value
of a brief job crafting intervention. Consider a situation where several
hundred workers must be hired into a firm. UA can be used to identify
the highest quality of workers that can be reasonably delivered to the
organization for a given cost (De Corte, 2011). An example is Sturman
(2000) who used UA to demonstrate that an enhanced employee program
described by a validity coefficient of .4 and applied to over 1400
applicants to hire 470 workers hired over several years is (after
accounting for a series of adjustments) expected to yield a median
return \~\$1.7M experienced over the lifetime use of the program.
Although large, this value represents only a few thousand dollars per
hire per year. Such economic value represents potential revenue
enhancements or cost savings attributed to, in this case, hiring
higher-performing workers (Schmidt, 2012).

Although UA is widely viewed as a complicated tool to both teach and
use, complicated tools in finance (e.g., the Black-Scholes equation)
also enjoy wide-spread use (Sturman, 2000). Russell (2022) notes that he
often builds web apps for IO psychology teams to use UA to aid
management decision making. What seems to be needed are tools that help
decision-makers (both IOs and non-IOs alike) make use of UA insights to
optimize investments more effectively in people.

With our manuscript, we make two contributions to the literature. First,
we synthesize the literature on communicating validity and utility
information to managers. Second, we build off both Cascio et al.'s
(2019) (who designed a web app made available in their *Investing in
People* textbook) and Russell (2022) (who designs web apps for UA in
practice) by introducing the UA+ web app, a tool designed to leverage
insights from literature to help IOs and business leaders communicate
the validity and utility of theory-derived interventions. In the
sections to follow, we summarize the literature informing the design of
our app, explain the capabilities that the UA+ application offers over
what is currently available, offer a tutorial grounded in the
literature, and conclude by discussing future directions for functions
built into the app. We hope the IO psychology community finds our app
useful while identifying ways to enhance it and that scholars use it to
teach students in applying UA to managerial decisions.

**A Review of the Utility Analysis (UA) Literature**

UA research goes back to the 1970s (see Boudreau, 1988). [The basic
utility model calculates utility as the dollar value of performance
differences resulting from an intervention (e.g. higher performance from
better selection or training) minus the costs of the intervention. The
general formula is below:]{.mark}

[ΔU = T \* N \* (Zbarx \* r \* SDy) -- C]{.mark}

[Where:]{.mark}

[ΔU = Utility change from selection device ;]{.mark}

[N = Number of people hired;]{.mark}

[T = Average tenure of those hired;]{.mark}

[Zbarx = Average Z-score of predictor for hired employees;]{.mark}

[r = Correlation between predictor and criterion (e.g.,
performance);]{.mark}

[SDy = Dollar value of a SD change in criterion;]{.mark}

[C = Cost of the intervention]{.mark}

[Unfortunately, this basic equation yields grossly inflated estimates of
economic value (e.g., over 14,000% return on investment; see Sturman,
2000).]{.mark}

Unsurprisingly, a 'futility of utility' research vein emerged positing
that presenting UA information to managers backfires (Latham & Whyte,
1994; Whyte & Latham, 1997). In a series of studies, Latham and
colleagues examined whether presenting utility information (e.g., the
economic value of enhanced staffing) supplemented the presentation of
validity information (e.g., the accuracy of the pre-employment staffing
test). Latham et al. found that while presenting validity information
was received favorably, presenting utility information alongside
validity information backfired, making managers hesitant to adopt a
valuable intervention. This narrative is discussed to this day (e.g.,
van Iddekinge et al., 2023) and for good reason; practitioners should be
rightly concerned about using methods that could backfire.

However, we should note that in explaining the 'futility of utility,'
Cronshaw (1997) (who was a contributor to the 'futility of utility'
research vein) theorized that the 'backfire effect' is attributable to a
'persuasional hypothesis'. Specifically, management is right to be
skeptical of overly complex methods for the problem at hand. Cronshaw
went on to posit that utility information can help managers make more
optimal decisions about practices that impact the workforce when it is
presented to inform rather than to persuade (Cronshaw, 1997).

Accordingly, later replication efforts showed more promise presenting
utility with validity information. Carson et al. (1998) found that
managers' acceptance of UA information was positively impacted by
utility information, although acceptance of the proposed intervention
was still distressingly low. Later work by Macan and Foster (2004) found
managers ranked UA information (e.g., the economic value of an
intervention) highly when making final decisions, stating the dollar
values helped in this. Later, Brooks et al. (2014) found using
non-traditional effect size indicators such as common language effect
size (CLES) helps managers understand the validity of interventions.

[We should note that scholars have provided guidance for making utility
estimates more accurate that include (i) accounting for economic factors
(i.e., interest, taxes, variable costs), (ii) (in the context of
selection) the cost of using multiple selection devices, deviating from
top-down hiring, and using a probationary period, and (iii) cohort or
temporal effects (e.g., validity over time) (see Sturman, 2000).
Although such adjustments can have a cumulative effect of shrinking
utility estimates to levels that are \~4% of the basic utility analysis
estimate and further complicate UA, they can help address flaws in
assumptions of the basic model and provide a more realistic valuation of
human resource programs for organizations. Unfortunately, managers'
receptivity to these methods has not been examined empirically.]{.mark}

In sum, we believe that used thoughtfully, UA can serve as a useful tool
to help business leaders make more informed decisions about investing in
people. Although it has been noted that UA can be helpful where IO
psychologists in their context (Russell, 2022), we believe that as a
discipline we can always improve how we help both business leaders and
academicians consider the implications of theory-based interventions.

**Introducing the Utility Analysis Plus (UA+) Tool**

We introduce the Utility Analysis plus (UA+) tool. Our work builds on
Cascio et al. (2019) by incorporating insights from validity and utility
analysis research that have either been shown to help managers make
decisions regarding how best to staff, train, and develop their talent
or are believed to support management decision-making (see Table 1).
Also is a glossary of terms for those less familiar with UA, and ways to
export the analysis in the .pdf format. Below, we offer a tutorial of
the app using three published examples: (i) the classic case of Latham
and Whyte (1994) (adapted by Carson et al., 1998), (ii) using UA to
estimate returns on leadership development investment (Avolio et al.,
2010), and (iii) using UA to communicate goal setting intervention value
(Schmidt, 2012). We do not cover all features of the app here as several
are still under development (e.g., using Monte Carlo analysis to
facilitate risk management). The app can be accessed here:
<https://uaplus.shinyapps.io/UAPlus/>.

**Improving Staffing: the Latham & Whyte (1994) Example**

Carson et al. (1998) demonstrate that the utility analysis conducted by
Latham and Whyte (1994) is useful in managers' decision-making if
presented in an easily understandable way. We reproduce key features of
Carson et al.'s vignettes with the UA+ app. Also, building on Cucina
(2017) and Latham and Whyte (1994), we add expectancy charts (Cucina,
2017) and plain-text explanations to aid decision making. To begin,
users should click on the Staffing Utility tab. There they will find an
introduction on staffing utility as well as the app, along with a
sidebar with sections, "Staffing Utility," "Expectancy," and "Utility
Outputs\'\' (see Figure 1).

When users click on the "Expectancy" section, they will be met with a
series of inputs used within the utility calculation. Default values for
these inputs are from Latham and Whyte (1994).[^1] Users will notice the
option of inputting costs and validity for separate procedures for
comparison. Results are populated when the "Compute Expectancy" button
is clicked, which consists of an expectancy chart (Cucina et al., 2017),
as well as a common language explanation of the expectancy results from
the user's inputs. This output also frames both the value added by
acquiring high performers and the opportunity cost of continuing to hire
poor performers (Hazer & Highhouse, 1997). Users will note that
throughout all outputs found within UA+, this framing effect is present
in visualization explanations, UA outputs, and UA plain-text
descriptions (see Figure 2).

Users may proceed to the "Utility Outputs" section. This section
contains an input panel with economic and workflow adjustments. Default
values for economic adjustments are from Sturman's (2000) while values
for workflows are from Cascio et al. (2019). Figure 3 shows the outputs
when the "Compute Utility" button is pressed.

These outputs consist of total unadjusted utility, total utility, per
year, and per hire per year dollar values of UA, a break-even value for
SDy, as well as a plain-text description of these values. The figure
shown for unadjusted utility reproduces the value found by Latham and
Whyte (1994), and plain-text descriptions are modeled similarly to those
by Carson et al. (1998).

**Improving Training or Employee Development**

**Case 1: the Avolio et al. (2010) example:** Avolio et al. demonstrate
how UA can be used in terms of training and be presented as the returns
on development investment (RODI). Our tool presents RODI using an effect
size visualization explaining how training impacts production and a
plain-text explanation in order to effectively explain results of RODI
analysis to managers. To use the RODI section of the app, users may
click on the "Training Utility" tab. This will lead to a page similar to
the "Staffing Utility" tab, with sidebar options of "Training Utility,"
"Effect Size," and "Utility Output." Clicking "Training Utility" will
display an explanation of training utility as well as explanations of
RODI and goal setting intervention (see Figure 4).

When users click on the "Effect Size" option, they will be met with an
input panel for displaying effect size. This panel includes a "Training
Effectiveness" input as well as options for training(RODI) or goal
setting. Users should click the training options, which will fill the
input boxes with a set of values found in Avolio et al. (2010)[^2] (see
Figure 5).

Figure 5 also displays the results of clicking the "Display Effect Size"
button. This output includes an effect size distribution chart
(Alexander et al., 2019) as well as a plain-language effect size
description (Magnusson, 2023; Hazer & Highhouse 1997). Users should then
navigate to the "Utility Outputs" section. There they will see a two
page input panel. These pages are similar to the "Staffing Utility"
panels, except that there are options for RODI and goal setting, along
with an input for "SD of Work Output". Users should click "RODI Inputs"
and then click "Calculate Utility (Unadjusted)" to display an unadjusted
utility estimate.

Users may then proceed to the second page of the panel to input utility
adjustments. This page contains inputs for economic adjustments and
workflows, as well as radio buttons to select output type and a "Compute
Utility" button. The defaults for the economic adjustments are from
Sturman (2000) while defaults for the workflow adjustments are from
Cascio et al. (2019). Figure 6 shows the output generated when "Compute
Utility" is clicked with the "Training Program" radio button selected.

This output consists of the values for RODI simply listed as well as in
the form of a plain-text description. The unadjusted utility value
reproduces the value found by Avolio et al. (2010)[^3]. A break even
value for SDy is also listed.

**Case 2. The Schmidt (2012) Example:** Schmidt (2012) uses utility
analysis in order to demonstrate the effectiveness of goal setting
interventions by presenting utility as a dollar value, as well as
presenting increases in production value from goal setting as a
reduction of labor cost. The UA+ tool is able to concisely present these
calculations using plain-text descriptions.

Using the "Effect Size" portion of the app for goal setting is identical
to the RODI method, except that goal setting inputs and outputs should
be selected. This will generate an output with terminology specific to
goal setting.

Users may then proceed to the "Utility Outputs" section and choose goal
setting inputs. This will enable the "SD of Work Output" input box which
is used in the calculation for increased production and reduced labor
costs. They may then press the "Compute Utility (Unadjusted)" button in
order to display the unadjusted utility in terms of dollar value from
goal setting. The second page of the panel and default adjustments are
identical to the previous tutorials. The "Goal Setting Output" radio
button should then be selected. Users should see inputs shown in Figure
7. The "Compute Utility" button displays returns on goal setting
procedures as well as production increases and labor reduction. The
unadjusted utility, increased production, and employee reduction values
reproduce the results found by Schmidt (2012).

The code for this app is available via Github:
https://github.com/utilityanalysis/webApp. Interested readers are
welcome to issue requests for the app or improve the app as they like
for their purposes.

**Discussion**

We have given a brief overview of the UA+ web app. In closing, we wish
to share our plans for (i) adding more features to the app and (ii)
engaging in a set of literal replications to identify those features
that meaningfully enhance the usefulness of this app for our community.

Features we plan to include follow. First, we wish to add graphical
descriptions of UA (i.e. slides) that allow a causal chain to be
described (e.g., % increase in training → % increase in performance → %
increase in economic value), which Winkler et al. (2010) suggest help
managers see how an intervention impacts the workforce. Second,
concerning the accuracy of UA, Sturman (2000) notes that a Monte Carlo
analysis allows a comprehensive set of adjustments to be used to
accurately appraise the value of HR interventions. We hope to update the
UA+ app with a Monte Carlo analysis feature that is relatively easy to
use and reproduces (as closely as possible) the findings from Sturman
(2000) (e.g., adjustments reduce basic utility estimates to \~4% of
their initial value). Third, we wish to augment the selection utility
tools in the app with a pareto-optimization feature that allows users to
identify the most economic value that can be attained while diversifying
the workforce (see De Corte et al., 2011; Rupp et al., 2020). Fourth, we
wish to highlight the neglected role of compensation in utility research
(e.g., its impact on workforce value, see Sturman, 2001, see Sturman,
2001). Our long-term aim is to have a tool that allows HR professionals
to identify optimal bundles of interventions (e.g., combinations of
staffing, training, and compensation enhancements).

We wish to close by noting that while our app's design is grounded in
the literature, more information or features in the app may not always
be better (Connelly et al., 2023); sometimes, less is more. Indeed, as
described in Table 1, there are many claims concerning how to present
evidence to managers, some of which have been supported. Therefore, our
first step is to replicate those effects that have been tested in the
literature via a set of literal replication studies (i.e., a replication
aimed at replicating, as close as possible, the initial conditions of
these studies). In this first wave of confirmatory testing, we will
identify and retain information features that are useful for our
community. As part of this, we will include conditions that test claims
that have gone relatively untested (e.g., framing effects applied to
validity). Our aim is to provide clear evidence concerning what does and
does not help users make decisions regarding investing in their people
and then build this information in the UA+ app. We have already begun
pre-registering our methods and hypotheses on the Open Science Framework
and look forward to incorporating feedback from our reviewers.

References

[Alexander III, L., Mulfinger, E., & Oswald, F. L. (2019). *Investing in
People Online* (Version 2.0) \[Software\], Rice University, Houston,
Texas. Available from <https://orgtools.shinyapps.io/IIP3/>]{.mark}

[Avolio, B. J., Avey, J. B., & Quisenberry, D. (2010). Estimating return
on leadership development investment. *The Leadership Quarterly, 21*(4),
633--644.
[https://doi.org/10.1016/j.leaqua.2010.06.006](https://psycnet.apa.org/doi/10.1016/j.leaqua.2010.06.006)]{.mark}

[Boudreau, J. W. (1988). *Utility Analysis for Decisions in Human
Resource Management* (CAHRS Working Paper #88-21). Ithaca, NY: Cornell
University, School of Industrial and Labor Relations, Center for
Advanced Human Resource Studies.
<https://ecommons.cornell.edu/items/915c3f50-e857-4a92-93e1-500b93b1b342>]{.mark}

[Brooks, M. E., Dalal, D. K., & Nolan, K. P. (2014). Are common language
effect sizes easier to understand than traditional effect sizes?
*Journal of Applied Psychology, 99*(2), 332--340.
<https://doi.org/10.1037/a0034745>]{.mark}

[Carson, K. P., Becker, J. S., & Henderson, J. A. (1998). Is utility
really futile? A failure to replicate and an extension. *Journal of
Applied Psychology, 83*(1), 84--96.
[https://doi.org/10.1037/0021-9010.83.1.84](https://psycnet.apa.org/doi/10.1037/0021-9010.83.1.84)]{.mark}

Cascio, W. F., Boudreau, J. W., & Fink, A. A. (2019). *Investing in
people: financial impact of human resource initiatives* (3rd ed.).
Society for Human Resource Management.

Cascio, W. F., & Morris, J. R. (1990). A critical reanalysis of Hunter,
Schmidt, and Coggin\'s (1988) \"Problems and pitfalls in using capital
budgeting and financial accounting techniques in assessing the utility
of personnel programs.\" *Journal of Applied Psychology, 75*(4),
410--417. <https://doi.org/10.1037/0021-9010.75.4.410>

Connelly, B. L., Ketchen, D. J., & Zhou, Y. S. (2023). The presenter's
paradox: More is not always better. *Journal of Management, 49*(7),
2208--2217. <https://doi.org/10.1177/01492063231155982>

Cronshaw, S.F. (1997), Lo! The stimulus speaks: The insider\'s view on
Whyte and Latham\'s "The futility of utility analysis". *Personnel
Psychology, 50*, 611-615.
<https://doi.org/10.1111/j.1744-6570.1997.tb00706.x>

Cucina, J., Berger, J., & Busciglio, H. (2017). Communicating
criterion-related validity using expectancy charts: A new approach.
*Personnel Assessment and Decisions*, *3*(1).
<https://doi.org/10.25035/pad.2017.001>

De Corte, W., Sackett, P. R., & Lievens, F. (2011). Designing
pareto-optimal selection systems: Formalizing the decisions required for
selection system development. *Journal of Applied Psychology*, *96*(5),
907--926. <https://doi.org/10.1037/a0023298>

Fisher, P. A., Risavy, S. D., Robie, C., König, C. J., Christiansen, N.
D., Tett, R. P., & Simonet, D. V. (2021). Selection myths: A conceptual
replication of HR professionals' beliefs about effective human resource
practices in the United States and Canada. *The Journal of Personnel
Psychology, 20*(2), 51--60. <https://doi.org/10.1027/1866-5888/a000263>

Götz, M., & O'Boyle, E. H. (2023). Cobblers, let's stick to our lasts! A
song of sorrow (and of hope) about the state of personnel and human
resource management science. In M. R. Buckley, A. R. Wheeler, J. E.
Baur, & J. R. B. Halbesleben (Eds.), *Research in Personnel and Human
Resources Management* (pp. 7--92). Emerald Publishing Limited.
<https://doi.org/10.1108/S0742-730120230000041004>

Hazer, J. T., & Highhouse, S. (1997). Factors influencing managers\'
reactions to utility analysis: Effects of SD~y~ method, information
frame, and focal intervention. *Journal of Applied Psychology, 82*(1),
104--112.
[https://doi.org/10.1037/0021-9010.82.1.104](https://psycnet.apa.org/doi/10.1037/0021-9010.82.1.104)

Highhouse, S. (2008). Stubborn reliance on intuition and subjectivity in
employee selection. *Industrial and Organizational Psychology, 1*(3),
333--342. <https://doi.org/10.1111/j.1754-9434.2008.00058.x>

Kaplan, R., & Norton, D. (2004). The strategy map: Guide to aligning
intangible assets. *Strategy & Leadership. 32*(5), 10-17.
<https://doi.org/10.1108/10878570410699825>

Latham, G. P., & Whyte, G. (1994). The futility of utility analysis.
*Personnel Psychology*, *47*(1), 31--46.
<https://doi.org/10.1111/j.1744-6570.1994.tb02408.x>

Macan, T. H., & Foster, J. (2004). Managers' reactions to utility
analysis and perceptions of what influences their decisions. *Journal of
Business and Psychology, 19*(2), 241--253.
<https://doi.org/10.1007/s10869-004-0550-x>

Magnusson, K. (2023, June 9). A causal inference perspective on
therapist effects. <https://doi.org/10.31234/osf.io/f7mvz>

Oprea, B. T., Barzin, L., Vîrgă, D., Iliescu, D., & Rusu, A. (2019).
Effectiveness of job crafting interventions: A meta-analysis and utility
analysis. *European Journal of Work and Organizational Psychology,
28*(6), 723--741. <https://doi.org/10.1080/1359432X.2019.1646728>

Pfeffer, J., & Fong, C. T. (2002). The end of business schools? Less
success than meets the eye. *Academy of Management Learning & Education,
1*(1), 78--95. <https://doi.org/10.5465/amle.2002.7373679>

Rousseau, D., & Boudreau, J. (2011). Sticky findings: Research evidence
practitioners find useful. In E., Lawler, & S., Mohrman (Eds.), *Useful
Research: Advancing Theory and Practice* (pp. 269-288). Berrett-Koehler.

Rupp, D., Song, Q. C., & Strah, N. (2020). Addressing the so-called
validity--diversity trade-off: Exploring the practicalities and legal
defensibility of Pareto-optimization for reducing adverse impact within
personnel selection. *Industrial and Organizational Psychology. 13*(2),
246-271. <https://doi.org/10.1017/iop.2020.19>

Russell, C. (2022). Scholarly course corrections needed to advance
organizational science: Field tests of theory-based deductions are long
overdue. In K. Murphy (ed.), *Data, Methods and Theory in the
Organizational Sciences: A New Synthesis* (pp. 241-265). Routledge.
<https://doi.org/10.4324/9781003015000>

> Rynes, S. L. (2012). The research-practice gap in I/O psychology and
> related fields: Challenges and potential solutions. In S. W. J.
> Kozlowski (Ed.), *The Oxford Handbook of Organizational Psychology,
> Vol. 1* (pp. 409--452). Oxford University Press.
> [https://doi.org/10.1093/oxfordhb/9780199928309.001.0001](https://psycnet.apa.org/doi/10.1093/oxfordhb/9780199928309.001.0001)

Rynes, S. L., Colbert, A. E., & Brown, K. G. (2002). HR Professionals'
beliefs about effective human resource practices: Correspondence between
research and practice. *Human Resource Management, 41*(2), 149--174.
https://doi.org/10.1002/hrm.10029

Rynes, S. L., Colbert, A. E., & O'Boyle, E. H. (2018). When the "Best
Available Evidence" Doesn't Win: How Doubts About Science and Scientists
Threaten the Future of Evidence-Based Management. *Journal of
Management*, *44*(8), 2995-3010.
<https://doi.org/10.1177/0149206318796934>

Schmidt, F. L. (2012). The economic value of goal setting to employers.
In E. A. Locke & G. P. Latham (Eds.), *New developments in goal setting
and task performance* (pp. 16--20). Routledge/Taylor & Francis Group.
<https://doi.org/10.4324/9780203082744>

Slade, L., Davenport, T., Roberts, D., & Shah, S. (2002). How Microsoft
optimized its investment in people after the dot‐com era. *Journal of
Organizational Excellence, 22*, 43 - 52.
<https://doi.org/10.1002/npr.10052>

Sturman, M. C. (2000). Implications of utility analysis adjustments for
estimates of human resource intervention value. *Journal of Management,
26*(2), 281--299. <https://doi.org/10.1177/014920630002600206>

Sturman, M. C. (2001). Utility analysis for multiple selection devices
and multiple outcomes. *Journal of Human Resource Costing & Accounting,
6*(2), 9--28. <https://doi.org/10.1108/eb029072>

Van Iddekinge, C. H., Lievens, F., & Sackett, P. R. (2023). Personnel
selection: A review of ways to maximize validity, diversity, and the
applicant experience. *Personnel Psychology, 76*, 651--686.
<https://doi.org/10.1111/peps.12578>

Whyte, G., & Latham, G. (1997). The futility of utility analysis
revisited: When even an expert fails. *Personnel Psychology, 50*(3),
601--610. <https://doi.org/10.1111/j.1744-6570.1997.tb00705.x>

Winkler, S., König, C. J., & Kleinmann, M. (2010). Single-attribute
utility analysis may be futile, but this can\'t be the end of the story:
Causal chain analysis as an alternative. *Personnel Psychology, 63*(4),
1041--1065. https://doi.org/10.1111/j.1744-6570.2010.01197.x

**Table 1**

*A literature review table summarizing insights on multiple utility
analysis (UA) attributes and their inclusion in either a widely-used
publicly available tool (Alexander et al., 2019) or the proposed UA+
app.*

  --------------------------------------------------------------------------
  UA Attribute             Expected Impact of UA    *Investing      UA+
                           Attribute on Managerial  in People*  
                           Reactions                  Online    
                                                    (Alexander  
                                                     et al.,    
                                                      2019)     
  ------------------------ ----------------------- ------------ ------------
  Validity                 **Positive** **but           ✓            ✓
                           weak**; Validity should              
                           help managers see that               
                           the practices work as                
                           claimed. While managers              
                           finding validity                     
                           evidence somewhat                    
                           helpful (Carson et al.,              
                           1998; Latham & Whyte,                
                           1994; Macan & Foster,                
                           2006; Whyte & Latham,                
                           1997), managers                      
                           nevertheless struggle                
                           to reason with validity              
                           information.                         

  Utility                  **Mixed**; Utility           ✓            ✓
                           should help managers                 
                           see whether the                      
                           intervention has                     
                           positive 'economic                   
                           value'. Presenting                   
                           utility alongside                    
                           validity backfires when              
                           used to persuade                     
                           managers (Latham &                   
                           Whyte, 1994; Whyte &                 
                           Latham, 1997); however,              
                           it is also viewed as                 
                           valuable when used to                
                           inform (Carson et al.,               
                           1998; Macan & Foster,                
                           2006).                               

  The economic value of    **Mixed**; SDy should        ✓            ✓
  improving the criterion  help managers                        
  (e.g., performance) by   appreciate the economic              
  one standard deviation   value of enhancing                   
  (SDy)                    performance. May give                
                           managers "an illusion                
                           of control" (p. 298)                 
                           because they can                     
                           calculate it themselves              
                           and leverages a                      
                           conservative tradition               
                           (i.e., the 40% rule)                 
                           (Highhouse, 1998);                   
                           despite leveraging                   
                           simplistic information,              
                           SDy may nevertheless                 
                           have no impact on                    
                           decision-making (Carson              
                           et al., 1998).                       

  Break-even investment    **Unclear**; Break-even      ✓            ✓
  levels                   values should help                   
                           managers to see the                  
                           minimum level of                     
                           economic value attached              
                           to performance                       
                           improvements that are                
                           needed to recover                    
                           costs. Presenting                    
                           break-even values of                 
                           SDy may help managers                
                           see the value of an                  
                           intervention in a term               
                           (break-even value) that              
                           they have already used               
                           in their work.                       

  Expectancy Charts        **Mixed**; Expectancy                     ✓
                           charts should help                   
                           managers see how                     
                           validity relates to the              
                           probability of hiring                
                           high performers or                   
                           avoiding bad hires.                  
                           Cucina (2017) maintains              
                           that expectancy charts               
                           have helped to                       
                           communicate validity to              
                           managers in practice;                
                           Latham and Whyte (1994)              
                           find that its use (in                
                           isolation or alongside               
                           utility) does not                    
                           impact management                    
                           reactions.                           

  Framing Effects;         **Unclear**; Framing                      ✓
  Avoiding a 'Toxic        utility as an                        
  Employee'                opportunity costs                    
                           (e.g., overlooking a                 
                           high potential                       
                           employee, unwittingly                
                           hiring a toxic worker)               
                           as opposed to a                      
                           monetary gain (e.g.,                 
                           hiring a                             
                           high-performing                      
                           employee, overlooking a              
                           toxic employee),                     
                           particularly for                     
                           manager who struggle to              
                           comprehend utility                   
                           (Hazer & Highhouse,                  
                           1997). While framing                 
                           utility has been                     
                           investigated (Hazer &                
                           Highhouse, 1997),                    
                           framing validity has                 
                           not been investigated.               

  Plain-Text Descriptions  **Positive**;                             ✓
  of Non-traditional       Presenting UA analysis               
  Effect Size Indicators   using a common language              
  (e.g., common language   effect size display                  
  effect size)             positively should help               
                           managers to see the                  
                           validity information in              
                           terms that they                      
                           understand. Brooks et                
                           al. (2014) found that                
                           managers were more                   
                           willing to pay for                   
                           training programs when               
                           given a common language              
                           effect size explanation              
                           (Brooks et al., 2014).               

  Impacts on Productivity, **Positive**;                             ✓
  Labor Costs              Illustrating how                     
                           interventions impact                 
                           workforce productivity               
                           and labor costs should               
                           help managers see UA as              
                           an optimization                      
                           tool(Cascio & Morris,                
                           1990; Schmidt, 2012).                
                           Macan and Foster (2006)              
                           find that managers are               
                           sensitive to cost                    
                           information presented                
                           as part of utility                   
                           analysis.                            

  Causal Chain Analysis OR **Positive;**                          Planned
  Graphical Depictions of  Presenting                           
  UA (i.e. slides)         interventions via a                  
                           causal chain diagram                 
                           positive should help                 
                           managers see how                     
                           interventions address                
                           workforce problems                   
                           (e.g., increasing                    
                           performance in a                     
                           crucial area of the                  
                           business).                           

  Monte Carlo Analysis as  **Unclear;** Presenting                   ✓
  a Comprehensive          a comprehensive set of               
  Adjustment and Risk      utility adjustments                  
  Management Strategy      should help managers                 
                           compute an accurate                  
                           utility estimate and                 
                           more effectively assess              
                           and manage risk.                     

  Impact of Compensation   **Unclear**;                          Discussing
  Enhancements on          Illustrating how                     
  Workforce Value          compensation                         
                           enhancements (e.g.,                  
                           strongly tying                       
                           incentives to                        
                           performance) impact                  
                           workforce quality,                   
                           quantity, and cost                   
                           should help managers                 
                           make decisions                       
                           regarding how                        
                           compensation systems                 
                           are designed.                        

  Identifying Options for  **Unclear;**                          Discussing
  Maximizing Validity,     Pareto-optimal decision              
  Utility, and Diversity   should help managers                 
  in Staffing              identify the maximum                 
                           level of utility that                
                           can be attained while                
                           diversifying the                     
                           workforce (DeCorte et                
                           al., 2011; Slade et                  
                           al., 2022; Rupp et al.,              
                           2020).                               
  --------------------------------------------------------------------------

**Figure 1**

![](media/image1.png){width="6.5in" height="3.5555555555555554in"}

**Figure 2**

![](media/image2.png){width="6.281435914260717in"
height="4.328125546806649in"}

**Figure 3**

![](media/image3.png){width="6.5in" height="4.472222222222222in"}

**Figure 4**

**d**![](media/image4.png){width="6.5in" height="2.4305555555555554in"}

**Figure 5**

![](media/image5.png){width="6.5in" height="2.861111111111111in"}

**Figure 6**

![](media/image6.png){width="6.1875in" height="5.013830927384077in"}

**Figure 7**

![](media/image7.png){width="6.5in" height="5.305555555555555in"}

[^1]: Inputs for selection ratio and new procedure cost per applicant
    were not found in Latham and Whyte but were derived from the cost
    portion of the utility equation. Values for validity were chosen to
    reflect real world examples more closely, and the difference of .4
    does reflect the value found in Latham and Whyte. The value of 200
    for old procedure cost per applicant was selected in order to obtain
    a difference close to 100, which is within the range described by
    Sturman (2000).

[^2]: These values are from the upper level leader estimates. Cost per
    employee is not directly provided by Avolio et al. but has been
    derived from the total utility results.

[^3]: After economic adjustments are applied, the cost of the Avolio
    case wipes out potential gains, highlighting Sturman's (2000)
    conclusion that HR interventions are context dependent.

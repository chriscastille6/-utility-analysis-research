---
title             : "Toward a More Useful Utility Analysis: A Literature Review and Web Application"
shorttitle        : "Utility Analysis Web App"

author: 
  - name          : "Christopher Castille"
    affiliation   : "1"
    corresponding : yes
    address       : "Department of Management, Nicholls State University, Thibodaux, LA"
    email         : "christopher.castille@nicholls.edu"
  - name          : "Eric Martinez"
    affiliation   : "1"
  - name          : "James Daigle"
    affiliation   : "1"
  - name          : "Kristen Toups"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Nicholls State University"

authornote: |
  Correspondence concerning this article should be addressed to Christopher Castille, Department of Management, Nicholls State University, Thibodaux, LA. Email: christopher.castille@nicholls.edu

abstract: |
  Scholars continue to discuss the merits and drawbacks of utility analysis (UA) as a tool for influencing management decisions. Despite its potential for quantifying the economic value of human resource interventions, UA has faced criticism for being overly complex and potentially backfiring when presented to managers. This paper synthesizes the literature on communicating validity and utility information to managers, introduces the UA+ web application, and demonstrates its usefulness through published case studies. The UA+ app incorporates evidence-based features including expectancy charts, plain-text descriptions, framing effects, and economic adjustments to enhance managerial decision-making. We illustrate the app's capabilities using three published examples: Latham and Whyte's (1994) staffing case, Avolio et al.'s (2010) leadership development analysis, and Schmidt's (2012) goal setting intervention. Our work contributes to bridging the science-practice gap by providing a practical tool that leverages research insights to help industrial-organizational psychologists and business leaders communicate the value of theory-based interventions.
  
keywords          : "utility analysis, human resource management, decision making, web application, science-practice gap, organizational psychology"
wordcount         : "2994"

bibliography      : "references.bib"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("references.bib")

# Load required libraries
library(knitr)
library(ggplot2)
library(dplyr)
library(kableExtra)
library(tidyr)
library(scales)

theme_set(theme_minimal() +
            theme(axis.text = element_text(size = 10),
                  axis.title = element_text(size = 12),
                  plot.title = element_text(size = 14, face = "bold"),
                  legend.text = element_text(size = 10)))
```

# Introduction

Scholars routinely bemoan the fact that management decisions are suboptimal [@fisher2021; @highhouse2008; @rynes2002]. Causes have been posited such as the poor training of managers [@highhouse2008; @rynes2012; @rynes2018], the widening science-practice gap [@rynes2012; @pfeffer2002], and fetish for theory in research that emphasizes complexity at the expense of validity and utility in our published research [@gotz2023]. To address these concerns, educators in human resource management and industrial-organizational psychology have been encouraged to improve their training of both practitioners and scholarly researchers using methods that emphasize validity and utility of theory-based interventions [@sturman2000; @sturman2001; @russell2022].

Utility analysis (UA) is one method of describing such evidence in statistical, economic, and heuristic terms (i.e., quantity, quality, and cost; @cascio2019). Recent examples of scholars using utility analysis to communicate the value of theory-based interventions include @avolio2010 who discuss the merits of a transformational leadership intervention; @schmidt2012 who highlights the economic impact of goal setting; and @oprea2019 who highlight the value of a brief job crafting intervention. Consider a situation where several hundred workers must be hired into a firm. UA can be used to identify the highest quality of workers that can be reasonably delivered to the organization for a given cost [@decorte2011]. An example is @sturman2000 who used UA to demonstrate that an enhanced employee program described by a validity coefficient of .4 and applied to over 1400 applicants to hire 470 workers hired over several years is (after accounting for a series of adjustments) expected to yield a median return ~$1.7M experienced over the lifetime use of the program. Although large, this value represents only a few thousand dollars per hire per year. Such economic value represents potential revenue enhancements or cost savings attributed to, in this case, hiring higher-performing workers [@schmidt2012].

Although UA is widely viewed as a complicated tool to both teach and use, complicated tools in finance (e.g., the Black-Scholes equation) also enjoy wide-spread use [@sturman2000]. @russell2022 notes that he often builds web apps for IO psychology teams to use UA to aid management decision making. What seems to be needed are tools that help decision-makers (both IOs and non-IOs alike) make use of UA insights to optimize investments more effectively in people.

With our manuscript, we make two contributions to the literature. First, we synthesize the literature on communicating validity and utility information to managers. Second, we build off both @cascio2019 (who designed a web app made available in their *Investing in People* textbook) and @russell2022 (who designs web apps for UA in practice) by introducing the UA+ web app, a tool designed to leverage insights from literature to help IOs and business leaders communicate the validity and utility of theory-derived interventions. In the sections to follow, we summarize the literature informing the design of our app, explain the capabilities that the UA+ application offers over what is currently available, offer a tutorial grounded in the literature, and conclude by discussing future directions for functions built into the app. We hope the IO psychology community finds our app useful while identifying ways to enhance it and that scholars use it to teach students in applying UA to managerial decisions.

# Method

## A Review of the Utility Analysis (UA) Literature

UA research goes back to the 1970s (see @boudreau1988). The basic utility model calculates utility as the dollar value of performance differences resulting from an intervention (e.g. higher performance from better selection or training) minus the costs of the intervention. The general formula is below:

$$\Delta U = T \times N \times (Z_{barx} \times r \times SD_y) - C$$

Where:

- $\Delta U$ = Utility change from selection device
- $N$ = Number of people hired
- $T$ = Average tenure of those hired
- $Z_{barx}$ = Average Z-score of predictor for hired employees
- $r$ = Correlation between predictor and criterion (e.g., performance)
- $SD_y$ = Dollar value of a SD change in criterion
- $C$ = Cost of the intervention

Unfortunately, this basic equation yields grossly inflated estimates of economic value (e.g., over 14,000% return on investment; see @sturman2000).

Unsurprisingly, a 'futility of utility' research vein emerged positing that presenting UA information to managers backfires [@latham1994; @whyte1997]. In a series of studies, Latham and colleagues examined whether presenting utility information (e.g., the economic value of enhanced staffing) supplemented the presentation of validity information (e.g., the accuracy of the pre-employment staffing test). Latham et al. found that while presenting validity information was received favorably, presenting utility information alongside validity information backfired, making managers hesitant to adopt a valuable intervention. This narrative is discussed to this day (e.g., @vaniddekinge2023) and for good reason; practitioners should be rightly concerned about using methods that could backfire.

However, we should note that in explaining the 'futility of utility,' @cronshaw1997 (who was a contributor to the 'futility of utility' research vein) theorized that the 'backfire effect' is attributable to a 'persuasional hypothesis'. Specifically, management is right to be skeptical of overly complex methods for the problem at hand. Cronshaw went on to posit that utility information can help managers make more optimal decisions about practices that impact the workforce when it is presented to inform rather than to persuade [@cronshaw1997].

Accordingly, later replication efforts showed more promise presenting utility with validity information. @carson1998 found that managers' acceptance of UA information was positively impacted by utility information, although acceptance of the proposed intervention was still distressingly low. Later work by @macan2004 found managers ranked UA information (e.g., the economic value of an intervention) highly when making final decisions, stating the dollar values helped in this. Later, @brooks2014 found using non-traditional effect size indicators such as common language effect size (CLES) helps managers understand the validity of interventions.

We should note that scholars have provided guidance for making utility estimates more accurate that include (i) accounting for economic factors (i.e., interest, taxes, variable costs), (ii) (in the context of selection) the cost of using multiple selection devices, deviating from top-down hiring, and using a probationary period, and (iii) cohort or temporal effects (e.g., validity over time) (see @sturman2000). Although such adjustments can have a cumulative effect of shrinking utility estimates to levels that are ~4% of the basic utility analysis estimate and further complicate UA, they can help address flaws in assumptions of the basic model and provide a more realistic valuation of human resource programs for organizations. Unfortunately, managers' receptivity to these methods has not been examined empirically.

In sum, we believe that used thoughtfully, UA can serve as a useful tool to help business leaders make more informed decisions about investing in people. Although it has been noted that UA can be helpful where IO psychologists in their context [@russell2022], we believe that as a discipline we can always improve how we help both business leaders and academicians consider the implications of theory-based interventions.

# Results

## Introducing the Utility Analysis Plus (UA+) Tool

We introduce the Utility Analysis plus (UA+) tool. Our work builds on @cascio2019 by incorporating insights from validity and utility analysis research that have either been shown to help managers make decisions regarding how best to staff, train, and develop their talent or are believed to support management decision-making (see Table 1). Also is a glossary of terms for those less familiar with UA, and ways to export the analysis in the .pdf format. Below, we offer a tutorial of the app using three published examples: (i) the classic case of @latham1994 (adapted by @carson1998), (ii) using UA to estimate returns on leadership development investment [@avolio2010], and (iii) using UA to communicate goal setting intervention value [@schmidt2012]. We do not cover all features of the app here as several are still under development (e.g., using Monte Carlo analysis to facilitate risk management). The app can be accessed here: https://uaplus.shinyapps.io/UAPlus/.

```{r literature-table, echo=FALSE}
library(knitr)
library(kableExtra)
lit_table <- data.frame(
  UA_Attribute = c("Validity", "Utility", "SDy", "Break-even levels",
                   "Expectancy Charts", "Framing Effects",
                   "Plain-Text Descriptions"),
  Expected_Impact = c("Positive but weak", "Mixed", "Mixed", "Unclear",
                      "Mixed", "Unclear", "Positive"),
  Key_References = c(
                     "Carson et al. (1998); Latham & Whyte (1994);
                     Macan & Foster (2004); Whyte & Latham (1997)",
                     "Latham & Whyte (1994); Whyte & Latham (1997); 
                     Carson et al. (1998); Macan & Foster (2004)",
                     "Highhouse (2008); Carson et al. (1998)",
                     "Cascio, Boudreau, & Fink (2019)",
                     "Cucina (2017); Latham & Whyte (1994)",
                     "Hazer & Highhouse (1997)",
                     "Brooks et al. (2014)")
)

kable(
  lit_table,
  format = "latex",
  booktabs = TRUE,
  col.names = c(
    "UA Attribute",
    "Expected Impact",
    "Key References"
  ),
  caption = paste(
    "Literature Review Summary of UA Attributes and",
    "Their Inclusion in Tools"
  ),
  longtable = TRUE
) %>%
  kable_styling(
    latex_options = c("hold_position", "repeat_header"),
    font_size = 8
  ) %>%
  column_spec(1, width = "2cm") %>%
  column_spec(2, width = "2.5cm") %>%
  column_spec(3, width = "10cm")
```

## Case Examples

### Improving Staffing: The Latham & Whyte (1994) Example

@carson1998 demonstrate that the utility analysis conducted by @latham1994 is useful in managers' decision-making if presented in an easily understandable way. We reproduce key features of Carson et al.'s vignettes with the UA+ app. Also, building on @cucina2017 and @latham1994, we add expectancy charts [@cucina2017] and plain-text explanations to aid decision making. To begin, users should click on the Staffing Utility tab. There they will find an introduction on staffing utility as well as the app, along with a sidebar with sections, "Staffing Utility," "Expectancy," and "Utility Outputs."

When users click on the "Expectancy" section, they will be met with a series of inputs used within the utility calculation. Default values for these inputs are from @latham1994. Users will notice the option of inputting costs and validity for separate procedures for comparison. Results are populated when the "Compute Expectancy" button is clicked, which consists of an expectancy chart [@cucina2017], as well as a common language explanation of the expectancy results from the user's inputs. This output also frames both the value added by acquiring high performers and the opportunity cost of continuing to hire poor performers [@hazer1997]. Users will note that throughout all outputs found within UA+, this framing effect is present in visualization explanations, UA outputs, and UA plain-text descriptions.

Users may proceed to the "Utility Outputs" section. This section contains an input panel with economic and workflow adjustments. Default values for economic adjustments are from @sturman2000 while values for workflows are from @cascio2019. The outputs consist of total unadjusted utility, total utility, per year, and per hire per year dollar values of UA, a break-even value for SDy, as well as a plain-text description of these values. The figure shown for unadjusted utility reproduces the value found by @latham1994, and plain-text descriptions are modeled similarly to those by @carson1998.

### Improving Training or Employee Development

**Case 1: The Avolio et al. (2010) Example:** @avolio2010 demonstrate how UA can be used in terms of training and be presented as the returns on development investment (RODI). Our tool presents RODI using an effect size visualization explaining how training impacts production and a plain-text explanation in order to effectively explain results of RODI analysis to managers. To use the RODI section of the app, users may click on the "Training Utility" tab. This will lead to a page similar to the "Staffing Utility" tab, with sidebar options of "Training Utility," "Effect Size," and "Utility Output." Clicking "Training Utility" will display an explanation of training utility as well as explanations of RODI and goal setting intervention.

When users click on the "Effect Size" option, they will be met with an input panel for displaying effect size. This panel includes a "Training Effectiveness" input as well as options for training(RODI) or goal setting. Users should click the training options, which will fill the input boxes with a set of values found in @avolio2010. The output includes an effect size distribution chart [@alexander2019] as well as a plain-language effect size description [@magnusson2023; @hazer1997]. Users should then navigate to the "Utility Outputs" section. There they will see a two page input panel. These pages are similar to the "Staffing Utility" panels, except that there are options for RODI and goal setting, along with an input for "SD of Work Output". Users should click "RODI Inputs" and then click "Calculate Utility (Unadjusted)" to display an unadjusted utility estimate.

Users may then proceed to the second page of the panel to input utility adjustments. This page contains inputs for economic adjustments and workflows, as well as radio buttons to select output type and a "Compute Utility" button. The defaults for the economic adjustments are from @sturman2000 while defaults for the workflow adjustments are from @cascio2019. The output consists of the values for RODI simply listed as well as in the form of a plain-text description. The unadjusted utility value reproduces the value found by @avolio2010. A break even value for SDy is also listed.

**Case 2: The Schmidt (2012) Example:** @schmidt2012 uses utility analysis in order to demonstrate the effectiveness of goal setting interventions by presenting utility as a dollar value, as well as presenting increases in production value from goal setting as a reduction of labor cost. The UA+ tool is able to concisely present these calculations using plain-text descriptions.

Using the "Effect Size" portion of the app for goal setting is identical to the RODI method, except that goal setting inputs and outputs should be selected. This will generate an output with terminology specific to goal setting. Users may then proceed to the "Utility Outputs" section and choose goal setting inputs. This will enable the "SD of Work Output" input box which is used in the calculation for increased production and reduced labor costs. They may then press the "Compute Utility (Unadjusted)" button in order to display the unadjusted utility in terms of dollar value from goal setting. The second page of the panel and default adjustments are identical to the previous tutorials. The "Goal Setting Output" radio button should then be selected. The "Compute Utility" button displays returns on goal setting procedures as well as production increases and labor reduction. The unadjusted utility, increased production, and employee reduction values reproduce the results found by @schmidt2012.

The code for this app is available via Github: https://github.com/utilityanalysis/webApp. Interested readers are welcome to issue requests for the app or improve the app as they like for their purposes.

# Discussion

We have given a brief overview of the UA+ web app. In closing, we wish to share our plans for (i) adding more features to the app and (ii) engaging in a set of literal replications to identify those features that meaningfully enhance the usefulness of this app for our community.

## Planned Features

Features we plan to include follow. First, we wish to add graphical descriptions of UA (i.e. slides) that allow a causal chain to be described (e.g., % increase in training → % increase in performance → % increase in economic value), which @winkler2010 suggest help managers see how an intervention impacts the workforce. Second, concerning the accuracy of UA, @sturman2000 notes that a Monte Carlo analysis allows a comprehensive set of adjustments to be used to accurately appraise the value of HR interventions. We hope to update the UA+ app with a Monte Carlo analysis feature that is relatively easy to use and reproduces (as closely as possible) the findings from @sturman2000 (e.g., adjustments reduce basic utility estimates to ~4% of their initial value). Third, we wish to augment the selection utility tools in the app with a pareto-optimization feature that allows users to identify the most economic value that can be attained while diversifying the workforce (see @decorte2011; @rupp2020). Fourth, we wish to highlight the neglected role of compensation in utility research (e.g., its impact on workforce value, see @sturman2001). Our long-term aim is to have a tool that allows HR professionals to identify optimal bundles of interventions (e.g., combinations of staffing, training, and compensation enhancements).

## Future Research Directions

We wish to close by noting that while our app's design is grounded in the literature, more information or features in the app may not always be better [@connelly2023]; sometimes, less is more. Indeed, as described in Table 1, there are many claims concerning how to present evidence to managers, some of which have been supported. Therefore, our first step is to replicate those effects that have been tested in the literature via a set of literal replication studies (i.e., a replication aimed at replicating, as close as possible, the initial conditions of these studies). In this first wave of confirmatory testing, we will identify and retain information features that are useful for our community. As part of this, we will include conditions that test claims that have gone relatively untested (e.g., framing effects applied to validity). Our aim is to provide clear evidence concerning what does and does not help users make decisions regarding investing in their people and then build this information in the UA+ app. We have already begun pre-registering our methods and hypotheses on the Open Science Framework and look forward to incorporating feedback from our reviewers.

# Conclusion

This paper has presented the UA+ web application as a practical tool for bridging the science-practice gap in utility analysis. By synthesizing the literature on communicating validity and utility information to managers, we have identified key features that enhance managerial decision-making, including expectancy charts, plain-text descriptions, framing effects, and economic adjustments. The UA+ app incorporates these evidence-based features to help industrial-organizational psychologists and business leaders communicate the value of theory-based interventions more effectively.

Our demonstration of the app using three published case studies—@latham1994 staffing analysis, @avolio2010 leadership development study, and @schmidt2012 goal setting intervention—illustrates the app's versatility across different HR interventions. The app successfully reproduces key findings from these studies while providing additional features that enhance comprehension and decision-making.

Future development of the UA+ app will focus on incorporating Monte Carlo analysis capabilities, pareto-optimization features, and compensation considerations. Additionally, planned replication studies will systematically test the effectiveness of various presentation features to ensure that the app provides optimal support for managerial decision-making.

The UA+ app represents a step toward addressing the persistent challenge of translating research insights into practical tools that can enhance organizational decision-making. By providing a user-friendly interface that incorporates evidence-based features, we hope to contribute to the broader goal of improving how organizations invest in their people and make evidence-based decisions about human resource interventions.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
